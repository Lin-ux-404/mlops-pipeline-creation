{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a training pipeline with Python function components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to your workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "\n",
    "subscription = os.environ[\"SUBSCRIPTION\"]\n",
    "resource_group = os.environ[\"RESOURCE_GROUP\"]\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "compute_cluster = os.environ[\"COMPUTE_CLUSTER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate and get a handle to the workspace\n",
    "\n",
    "Use the Azure ML SDK to authenticate and get a handle to the workspace using the provided subscription ID, resource group name, and workspace name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription,\n",
    "    resource_group_name=resource_group,\n",
    "    workspace_name=ws_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data store and upload training data\n",
    "\n",
    "Create a data store in the Azure ML workspace and upload the training data from the local directory. You only need to run this once. Once the data has been uploaded, you only have to run the cell after this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "data_path = \"../data\"\n",
    "dataset_name = \"diabetes-data-train\"\n",
    "\n",
    "patient_dataset = Data(\n",
    "    path=data_path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Training data for diabetes prediction\",\n",
    "    name=dataset_name,\n",
    ")\n",
    "ml_client.data.create_or_update(patient_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Retrieve the uploaded dataset from the Azure ML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_dataset = ml_client.data.get(\n",
    "    name=\"diabetes-data-train\", label=\"latest\"\n",
    ")\n",
    "\n",
    "print(patient_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline components\n",
    "\n",
    "Define the functions that will be used as components in the pipeline. We need to import the components from the python files first. Run the directory settings only once to set the project root root relative to the current notebook's location. Confirm the project_root is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine the project root relative to the current notebook's location\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent  # Adjust to go one level up to the project root\n",
    "\n",
    "# Change the working directory to the project root\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from py_func_based_pipeline.components.compare_models import compare_two_models\n",
    "from py_func_based_pipeline.components.fix_missing_data import remove_empty_rows\n",
    "from py_func_based_pipeline.components.normalize_data import normalize_data\n",
    "from py_func_based_pipeline.components.train_decision_tree import train_decision_tree_classifier_model\n",
    "from py_func_based_pipeline.components.train_logistic_regression import train_logistic_regression_classifier_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can upload the components to AML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.components.create_or_update(remove_empty_rows)\n",
    "ml_client.components.create_or_update(normalize_data)\n",
    "ml_client.components.create_or_update(train_logistic_regression_classifier_model)\n",
    "ml_client.components.create_or_update(train_decision_tree_classifier_model)\n",
    "ml_client.components.create_or_update(compare_two_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def diabetes_classification_pipeline_component( pipeline_job_input: Input):\n",
    "    clean_data = remove_empty_rows(input_data=pipeline_job_input)\n",
    "    normalized_data = normalize_data(input_data=clean_data.outputs.output_data)\n",
    "    train_model_decision_tree = train_decision_tree_classifier_model(training_data=normalized_data.outputs.output_data)\n",
    "    train_model_logistic_regression = train_logistic_regression_classifier_model(training_data=normalized_data.outputs.output_data)\n",
    "    better_model = compare_two_models(model1=train_model_decision_tree.outputs.model_output_decision_tree, \n",
    "                                            model1_metrics=train_model_decision_tree.outputs.metrics_output,\n",
    "                                            model2=train_model_logistic_regression.outputs.model_output_logistic_reg,\n",
    "                                            model2_metrics=train_model_logistic_regression.outputs.metrics_output)                             \n",
    "    \n",
    "    return {\n",
    "        \"pipeline_job_transformed_data\": normalized_data.outputs.output_data,\n",
    "        \"pipeline_job_trained_model_decision_tree\": train_model_decision_tree.outputs.model_output_decision_tree,\n",
    "        \"pipeline_job_trained_model_logistic_regression\": train_model_logistic_regression.outputs.model_output_logistic_reg,\n",
    "        \"pipeline_job_better_model\": better_model.outputs.better_model,\t\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the pipeline component to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_train_pipeline_component = ml_client.components.create_or_update(\n",
    "    diabetes_classification_pipeline_component\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the registered pipeline component to create a pipeline job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = registered_train_pipeline_component(pipeline_job_input=Input(type=AssetTypes.URI_FILE, path=patient_dataset.path))\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = compute_cluster\n",
    "# set pipeline level datastore\n",
    "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
    "\n",
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_diabetes_training_py_func\"\n",
    ")\n",
    "pipeline_job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
